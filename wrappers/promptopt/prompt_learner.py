import torch
import torch.nn as nn

from core.vision_encoder import tokenizer


class VisionPromptLearner(nn.Module):
    def __init__(self, num_prompt: int, emb_size: int, is_cls_present: bool = False):
        super().__init__()
        self.num_prompt = num_prompt
        self.emb_size = emb_size        
        self.prompt_tokens = nn.Parameter(torch.randn(num_prompt, emb_size))
        self.is_cls_present = is_cls_present

    def forward(self, x):
        batch_size = x.shape[0]

        cntx = self.prompt_tokens.unsqueeze(0).expand(batch_size, -1, -1) # [B, num_prompt, D]
        if not self.is_cls_present:
            x = torch.cat([cntx, x], dim=1)
        else:
            x = torch.cat([x[:, :1], cntx, x[:, 1:]], dim=1)

        return x
    

class PromptLearner(nn.Module):
    '''
        This class takes as input the context generated by the prompt_gen class for the specific task and computes
        the prompts for the class of the task. The prompt are computed using the text model and the tokenizer.
        The tokenizer is used to encode the class names and the context placeholders "X" into, after that the embedding layer
        of the text model is used to compute the embeddings for the class names and the context placeholders.
        The embedded context placeholders are then removed and the class name embeddings are concatenated with the context embeddings 
        of the task to form the final prompt.

        tokenizer("x x x x x x male") -> [SOS, ph1, ph2, ..., ph_n, CLSTOKEN1, ..., CLSTOKENp, EOS] (where ph1, ph2, ..., ph_n are the context placeholders and CLSTOKEN1, ..., CLSTOKENp are the class name tokens.)
        textModel.token_embedding([SOS, ph1, ph2, ..., ph_n, CLSTOKEN1, ..., CLSTOKENp, EOS]) -> [SOS_EMBEDDING, ph1_EMBEDDING, ph2_EMBEDDING, ..., ph_n_EMBEDDING, CLSTOKEN1_EMBEDDING, ..., CLSTOKENp_EMBEDDING, EOS_EMBEDDING]
        The final prompt is then computed as follows:
        prefix = SOS_EMBEDDING
        suffix = [CLSTOKEN1_EMBEDDING, ..., CLSTOKENp_EMBEDDING, EOS_EMBEDDING]

        prompts = [PREFIX, TASK_CONTEXT, SUFFIX]]

    '''
    def __init__(self, n_ctx, classnames, textModel, tokenizer, verbose=False):
        '''
        n_ctx: number of context vector to be used in the prompt, must be equal to the number of task context vectors generated by the metanet
        classnames: list of class names for the task, used to compute the prompts
        textModel: text model used to compute the embeddings for the class names and the context placeholders
        tokenizer: tokenizer used to encode the class names and the context placeholders        
        '''

        super().__init__()
        n_cls = len(classnames) 
        dtype = textModel.get_token_embedding_layer().weight.dtype  # Get the dtype from the text model

        # Generate the prompt placeholders for the tokenizer
        prompt_prefix = " ".join(["X"] * n_ctx)
        if verbose:
            print(f'Initial context: "{prompt_prefix}"')
            print(f"Number of context words (tokens): {n_ctx}")

        classnames = [name.replace("_", " ") for name in classnames] # Replace underscores with spaces in class names
        prompts = [prompt_prefix + " " + name + "." for name in classnames] # Generate prompts for each class name using the context placeholders and the class names

        tokenized_prompts = torch.cat([tokenizer(p) for p in prompts]) # Tokenize the prompts using the tokenizer for all the classes
        with torch.no_grad():
            embedding = textModel.get_token_embedding_layer()(tokenized_prompts).type(dtype) # Compute the embeddings for the tokenized prompts using the text model

        
        self.register_buffer("token_prefix", embedding[:, :1, :])          # SOS
        self.register_buffer("token_suffix", embedding[:, 1 + n_ctx:, :])  # CLS, EOS

        self.n_cls = n_cls
        self.n_ctx = n_ctx
        self.tokenized_prompts = tokenized_prompts
        
    def forward(self, ctx, idx=None):
        """
        ctx: context generated by prompt_gen
        returns: prompts for the class of the task [n_cls, n_ctx, emb_size]
        """

        if ctx.dim() == 2:
            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)

        prefix = self.token_prefix
        suffix = self.token_suffix

        if idx is not None:
            prefix = prefix[idx]
            suffix = suffix[idx]

        prompts = torch.cat([prefix, ctx, suffix], dim=1)

        return prompts
    
class TaskPromptLearner(nn.Module):
    '''
        This class is used to learn the context vectors for the specific task.
    '''
    def __init__(self, n_ctx, tasknames, text_model, tokenizer, verbose=False):
        super().__init__()
        n_task = len(tasknames)
        dtype = text_model.get_token_embedding_layer().weight.dtype  # Get the dtype from the text model
        ctx_dim = text_model.width
        
         # Generate n_ctx context vectors for the each task
        ctx_vectors = torch.empty(n_task, n_ctx, ctx_dim, dtype=dtype)
        nn.init.normal_(ctx_vectors, std=0.02)

        prompt_prefix = " " + " ".join(["X"] * n_ctx) # Generate the prompt placeholders for the tokenizer

        if verbose:
            print(f'Initial context: "{prompt_prefix}"')
            print(f"Number of context words (tokens): {n_ctx}")

        if n_ctx == 0:
            self.register_buffer("ctx", ctx_vectors)  # not optimized
        else:
            self.register_parameter("ctx", nn.Parameter(ctx_vectors))  # to be optimized

        tasknames = [name.replace("_", " ") for name in tasknames]          # Replace underscores with spaces in task names
        prompts = [prompt_prefix + " " + name + "." for name in tasknames]  # Concatenate the placeholders with the task names

        tokenized_prompts = torch.cat([tokenizer(p) for p in prompts])      # Tokenize the prompts using the tokenizer for all the tasks
        with torch.no_grad():
            embedding = text_model.get_token_embedding_layer()(tokenized_prompts).type(dtype) # Compute the embeddings for the tokenized prompts using the text model


        # NOTE: it allows to access token_prefix and token_suffix by self.token_prefix and self.token_suffix in a static way
        self.register_buffer("token_prefix", embedding[:, :1, :])  # SOS
        self.register_buffer("token_suffix", embedding[:, 1 + n_ctx:, :])  # CLS, EOS

        self.n_task = n_task
        self.n_ctx = n_ctx
        self.tokenized_prompts = tokenized_prompts # Contains the tokenized prompts for all the tasks with the relative context placeholders

    def forward(self):
        '''
            It does not take any input, because all the information is stored in the class attributes.
            It returns the prompts for the task, which are computed using the context vectors and the task names used to initialize the class.
            The output is a tensor of shape (n_task, n_ctx + n_task_token, dim)
        '''
        ctx = self.ctx
        if ctx.dim() == 2:
            ctx = ctx.unsqueeze(0).expand(self.n_task, -1, -1)

        prefix = self.token_prefix
        suffix = self.token_suffix # NOTE: The suffix contain the embeddings for the task and the EOS tokens
        
        prompts = torch.cat([prefix, ctx, suffix], dim=1)  # Concatenate the prefix, context and suffix to form the final prompts

        return prompts


class PromptGen(nn.Module):
    """
    prompt generator, Used to generate the context using the MetaNeta described in the paper
    """

    def __init__(self, gen_type, n_ctx, feat_dim, out_dim, dtype):
        super().__init__()

        self.dtype = dtype
        self.n_ctx = n_ctx
        self.out_dim = out_dim
        self.prompt_gen_type = gen_type

        if self.prompt_gen_type == "lin":
            width = feat_dim
            self.register_parameter("proj", nn.Parameter(torch.zeros(width, n_ctx * out_dim))) # Matrix to project the features to the context dimension
            nn.init.normal_(self.proj, std=0.02)
        elif self.prompt_gen_type == "mlp":
            raise NotImplementedError("MLP prompt generation is not implemented yet") # TODO: Implement MLP prompt generation
        else:
            raise NotImplementedError

    def forward(self, x):
        '''
            x: task features obtained from the text model
            returns: prompt context for the task to be prepende to the class embeddings
        '''
        ctx = None
        if self.prompt_gen_type == "lin":
            ctx = x @ self.proj.type(x.dtype)
            ctx = ctx.reshape(x.shape[0], self.n_ctx, self.out_dim) # [B, n_ctx, EMB_SIZE]
        elif self.prompt_gen_type == "mlp":
            raise NotImplementedError("MLP prompt generation is not implemented yet") # TODO: Implement MLP prompt generation        

        return ctx

class CustomModel(nn.Module):
    '''
        Custom model that combines the vision encoder and the text encoder with a prompt learner.
        To be used the model passed as parameters must have the following attributes:
        - visual: the vision encoder
        - text_model: the text encoder with the following attributes and methods:
            - prompt_forward: method to compute the text features given the prompts and the tokenized prompts
            - width: the width of the text model
        - logit_scale: the logit scale factor
        - dtype: the data type of the model (e.g. torch.float32)

    '''
    def __init__(self, n_ctx, tasknames, classnames, model, tokenizer):
        super().__init__()        
        self.prompt_learner = nn.ModuleList([PromptLearner(n_ctx, c, model.text_model, tokenizer) for c in classnames])
        self.task_prompt_learner = TaskPromptLearner(n_ctx, tasknames, model.text_model, tokenizer)
        self.task_tokenized_prompts = self.task_prompt_learner.tokenized_prompts

        self.model = model    
        self.image_encoder = model.visual if hasattr(model, "visual") else model.vision_model
        self.text_model = model.text_model
        
        self.logit_scale = model.logit_scale if hasattr(model, 'logit_scale') else torch.ones(1, dtype=torch.float32).cuda()
        self.logit_bias = model.logit_bias if hasattr(model, 'logit_bias') else torch.zeros(1, dtype=torch.float32).cuda()
        self.dtype = model.dtype if hasattr(model, 'dtype') else torch.float32
        self.image_size = model.image_size if hasattr(model, 'image_size') else 224
        self.n_ctx = n_ctx
        self.ctx_dim = model.text_model.width if hasattr(model.text_model, 'width') else 768
        self.task_feat_dim = model.text_model.width if hasattr(model.text_model, 'width') else 768

        self.prompt_gen = PromptGen(
            'lin',
            self.n_ctx,
            self.task_feat_dim,
            self.ctx_dim,
            self.dtype
        )

        self.num_classes = [len(x) for x in classnames]        
    
    def get_softCPT_parameters(self):
        """
        Returns the parameters of the prompt learner and the task prompt learner.
        """
        return list(self.prompt_learner.parameters()) + list(self.task_prompt_learner.parameters()) + list(self.prompt_gen.parameters())

    def forward(self, image):

        prompts = self.task_prompt_learner()                                # Create the prompt for the task using the task prompt learner
        task_tokenized_prompts = self.task_tokenized_prompts                # Get the tokenized prompts for the task

        # TODO: check on the git implemetation the use of task_tokenized_prompts
        task_features = self.text_model.prompt_forward(prompts, task_tokenized_prompts)  # Compute the task features using the text encoder and the tokenized prompts

        
        ctx = self.prompt_gen(task_features.type(self.dtype))                # Generate the context to be concatenated with the class embeddings

        image_features = self.image_encoder(image.type(self.dtype))

        text_features = self._compute_text_features_full(ctx)                # Compute the text features for all the classes using the context generated by the prompt generator

        image_features = image_features / image_features.norm(dim=-1, keepdim=True) # Normalize the image features
        text_features = text_features / text_features.norm(dim=-1, keepdim=True)    # Normalize the text features


                
        logits = self.logit_scale.exp() * image_features @ text_features.t()  + self.logit_bias               # Compute the logits using the image features and the text features

        return logits

    def _compute_text_features_full(self, ctx):
        '''
            Compute the text features for all classes using the right task context
        '''
        text_features = []
        for i, p in enumerate(self.prompt_learner):
            # The i-th prompt learner is used to compute the text features for the classes of the i-th task
            # ctx[i] is the context for the i-th task
            prompts = p.forward(ctx[i])
            f = self.text_model.prompt_forward(prompts, p.tokenized_prompts)
            text_features.append(f)
        # Concatenate the text features for all classes
        text_features = torch.cat(text_features, dim=0)
        return text_features

    def get_text_features(self, normalize: bool = False):
        prompts = self.task_prompt_learner()                                # Create the prompt for the task using the task prompt learner
        task_tokenized_prompts = self.task_tokenized_prompts                # Get the tokenized prompts for the task

        # DONE: check on the git implemetation the use of task_tokenized_prompts
        # task tokenized prompts are the token computed over the task name and the context placeholders
        task_features = self.text_model.prompt_forward(prompts, task_tokenized_prompts)  # Compute the task features using the text encoder and the tokenized prompts

        
        ctx = self.prompt_gen(task_features.type(self.dtype))                # Generate the context to be concatenated with the class embeddings

        text_features = self._compute_text_features_full(ctx)                # Compute the text features for all the classes using the context generated by the prompt generator
        if normalize:
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        return text_features

    def get_image_features(self, image, normalize: bool = False):
        image_features = self.image_encoder(image.type(self.dtype))
        if normalize:
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
        return image_features
    
    def save_text_features(self, text_features_path: str, normalize: bool = True):
        """
        Calcola e salva le text features (normalizzate) pronte per essere caricate da PECore_Vision.
        """
        import os
        os.makedirs(os.path.dirname(text_features_path), exist_ok=True)
        text_features = self.get_text_features(normalize=normalize).detach().cpu()
        torch.save({'text_features': text_features}, text_features_path)
        print(f"[CustomModel] Text features salvate in: {text_features_path} (shape: {text_features.shape})")

    def save_for_training(self, save_path: str):
        """
        Salva tutto il modello (inclusi tutti i pesi dei vari prompt learner e task prompt learner)
        per poter riprendere l'addestramento dai pesi appresi.
        """
        import os
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        torch.save(self.state_dict(), save_path)
        print(f"[CustomModel] Modello completo salvato per training in: {save_path}")